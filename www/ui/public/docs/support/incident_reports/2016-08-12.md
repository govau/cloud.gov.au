## August 12, 2016 incident report

On August 12, 2016 there was approximately 79 minutes of partial availability for dto.gov.au, australia.gov.au, gov.au/alpha, gov.au/marketplace. During this time, approximately a third of visitors to these sites were unable to view them.

The cloud.gov.au team takes the availability of these sites seriously, and we are really sorry for the degraded service. While we rely on our suppliers to provide the infrastructure to deliver services, we own our availability, and we take full responsibility for the degraded service.

This report explains what we’ve learnt about what happened, what factors contributed to the outage, and what we’re doing to prevent similar incidents occurring in the future.

### What we learnt

The cloud.gov.au service is made of two components: a Platform as a Service (PaaS) for running applications, and a HTTPS termination layer (httpredir) for exposing these applications to the public internet.

httpredir issues redirects for dto.gov.au and australia.gov.au, serves the www.gov.au, www.gov.au/alpha, and gov.au/marketplace sites, and performs SSL termination on these sites. We do this because we must support older web browsers and operating systems that don’t support more modern methods for serving content.
Because of these constraints, we use a very simplistic form of load balancing called Round Robin DNS to make sure people can access our services in a variety of degraded conditions.

When using Round Robin DNS, a web browser has a choice of different addresses it can connect to when requesting content. The web browser will rotate through these addresses when making requests. This ensures that if a request to one address fails, a user can still get content by manually hitting the refresh button in their browser.

The httpredir service runs in three availability zones in ap-southeast-2 (2a, 2b, and 2c), and is scaled with Auto Scaling Groups.

Sites and services hosted on httpredir were partially unavailable because of a localised network failure in ap-southeast-2c. This caused connections from web browsers to time out when trying to access sites provided by httpredir.

We observed the localised network failure started at approximately 10.16, and was fixed by AWS at approximately 11.35.

### Contributing factors

After conducting a Post Incident Review, we have determined the following factors contributed to the degraded service:

 - We opted for Round Robin DNS as a load balancing technique over other load balancing techniques because we believed it struck the right balance between user experience and architectural complexity. In the case of this particular failure, user experience lost out.
 - A routing failure on AWS’s network caused large volumes of connections to the httpredir instance in ap-southeast-2c to time out.
 - The routing failure only affected instances in ap-southeast-2c that were exposed to the internet with an Elastic IP (EIP). All of our httpredir instances have EIPs assigned.

### What we're doing differently

We see service failure like these as both an excellent opportunity to learn about how complex systems fail, and how we can improve the reliability and quality of service the cloud.gov.au team provides.

 - We will add an early step to our incident response run sheet to decide if we should escalate to suppliers of technology we use. The intention is to reduce the lead time on responses, and help us build context quicker.
 - We intend to change how we share output from network diagnostic tools like `mtr` to include the command as well as the output, so everyone working the incident can easily reproduce the steps to troubleshoot.
 - We intend to disable alerts for some monitoring checks. As we experience more failures, we are learning more about what monitoring checks are useful indicators of whole of service issues that require human intervention, and what are diagnostic checks that help troubleshoot incidents.
 - We will investigate setting up canaries across multiple locations around the world to constantly collect network performance metrics. This information will help us establish a network performance baseline more quickly during outages.
 - We will more clearly define in our incident response run sheet when outwards communication should happen, to ensure all service owners receive timely information about degraded services.

### Conclusion

All of us on the cloud.gov.au team apologise for the impact of this outage. We continue to analyse the events leading up to this incident and the steps we took to restore service. This work will guide us as we improve the systems, tools, and processes that power cloud.gov.au.

If you have any questions, please don't hesitate to contact the cloud.gov.au team at support@cloud.gov.au.
